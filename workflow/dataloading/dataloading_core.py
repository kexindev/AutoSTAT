import csv
import io
import os
from typing import List, Optional

import chardet
import numpy as np
import pandas as pd
from scipy import sparse
from scipy.io import loadmat, arff
import streamlit as st
import streamlit_antd_components as sac


def read_data_from_file(
    uploaded_data_file,
    col_names: Optional[List[str]] = None,
    sep: Optional[str] = None,
    na_values: List[str] = ['?'],
    encoding: Optional[str] = None
) -> pd.DataFrame:
    """
    ä»ä¸Šä¼ çš„æ•°æ®æ–‡ä»¶è¯»å– DataFrameã€‚
    - æ”¯æŒ .csv/.data/.txt/.xlsx/.xls/.mat
    - col_names=None æ—¶ä½¿ç”¨ header=0ï¼ˆæ–‡ä»¶é¦–è¡Œåšåˆ—åï¼‰
    - col_names ä¸ä¸º None æ—¶ä½¿ç”¨ header=None å¹¶æŒ‡å®š names=col_names
    - æ–‡æœ¬æ–‡ä»¶ï¼šè‡ªåŠ¨æ¢æµ‹ç¼–ç ã€å—…æ¢åˆ†éš”ç¬¦ï¼Œè·³è¿‡åè¡Œ
    - Excel æ–‡ä»¶ï¼šç›´æ¥ä½¿ç”¨ pandas.read_excel
    - MAT æ–‡ä»¶ï¼šä½¿ç”¨ scipy.loadmatï¼Œæå–ç¬¬ä¸€ä¸ªä¸»è¦å˜é‡ï¼Œè½¬ä¸º DataFrameï¼Œå¹¶ä¿è¯ä¸€ç»´åˆ—
    """
    # è¯»å–æ‰€æœ‰å­—èŠ‚
    data_bytes = uploaded_data_file.read()
    # é‡ç½®æµä½ç½®
    try:
        uploaded_data_file.seek(0)
    except Exception:
        pass

    name = uploaded_data_file.name
    ext = os.path.splitext(name)[1].lower()

    # Excel æ–‡ä»¶å¤„ç†
    if ext in ('.xlsx', '.xls'):
        excel_kwargs = {}
        if col_names is None:
            excel_kwargs['header'] = 0
        else:
            excel_kwargs['header'] = None
            excel_kwargs['names'] = col_names
        return pd.read_excel(io.BytesIO(data_bytes), **excel_kwargs)

    # ARFF æ–‡ä»¶ç‰¹æ®Šå¤„ç†
    if ext == '.arff':
        text = data_bytes.decode(encoding or 'utf-8', errors='ignore')
        raw_data, meta = arff.loadarff(io.StringIO(text))
        df = pd.DataFrame(raw_data)
        for col in df.select_dtypes([object]).columns:
            if isinstance(df[col].iloc[0], bytes):
                df[col] = df[col].str.decode('utf-8', errors='ignore')
        if col_names is not None and df.shape[1] == len(col_names):
            df.columns = col_names
        return df
        
    # â€”â€” MAT æ–‡ä»¶ç‰¹æ®Šå¤„ç† â€”â€” #
    if ext == '.mat':
        mat = loadmat(io.BytesIO(data_bytes))
        data_keys = [k for k in mat.keys() if not k.startswith('__')]
        if not data_keys:
            raise ValueError('MAT æ–‡ä»¶ä¸­æœªå‘ç°æœ‰æ•ˆæ•°æ®å˜é‡')
        arr = mat[data_keys[0]]

        # â€”â€” å…ˆå¤„ç†ç¨€ç–çŸ©é˜µ â€”â€” #
        if sparse.issparse(arr):
            arr = arr.toarray()

        arr = np.array(arr)
        if arr.ndim > 2:
            arr = arr.reshape(arr.shape[0], -1)

        df = pd.DataFrame(arr)

        if col_names is not None and df.shape[1] == len(col_names):
            df.columns = col_names

        return df

    if encoding is None:
        det = chardet.detect(data_bytes)
        encoding = det.get("encoding", "utf-8")

    if encoding.lower() in ("utf-16", "utf-16le", "utf-16be", 
                            "utf-32", "utf-32le", "utf-32be"):
        text = data_bytes.decode(encoding, errors="ignore")
        data_bytes = text.encode("utf-8")
        encoding = "utf-8"

    sample = data_bytes[:10000].decode(encoding, errors="ignore")

    first_line = sample.splitlines()[0].strip()

    if sep is not None:
        detected_sep = sep
        use_whitespace = False

    elif "," in first_line:
        detected_sep = ","
        use_whitespace = False

    else:
        try:
            dialect = csv.Sniffer().sniff(
                sample,
                delimiters=[",", ";", "\t", "|"]
            )
            detected_sep = dialect.delimiter
            use_whitespace = False
        except csv.Error:
            detected_sep = None
            use_whitespace = True  # fallback

    read_kwargs = {
        "engine": "python",
        "encoding": encoding,
        "na_values": na_values,
        "skipinitialspace": True,
        "on_bad_lines": "skip",
    }

    if col_names is None:
        read_kwargs["header"] = 0
    else:
        read_kwargs["header"] = None
        read_kwargs["names"] = col_names

    if use_whitespace:
        read_kwargs["delim_whitespace"] = True
    else:
        read_kwargs["sep"] = detected_sep

    return pd.read_csv(io.BytesIO(data_bytes), **read_kwargs)


def process_complex_data(uploaded_files, dataloadingagent):
    """
    ä¸Šä¼ å¤„ç†é€»è¾‘ï¼š
    - å•æ–‡ä»¶ï¼šå½“ä½œæ™®é€šè¡¨æ ¼æˆ– MAT æ–‡ä»¶è¯»ï¼ˆç¬¬ä¸€è¡Œå½“è¡¨å¤´ï¼‰
    - å¤šæ–‡ä»¶ï¼šåˆ†åˆ«è¯»å–æ¯ä¸ªæ–‡ä»¶ï¼Œä¿æŒå„è‡ªçš„åˆ—åå’Œæ ¼å¼
      ä¸å¼ºåˆ¶æ‹¼æ¥ï¼Œç”±ç”¨æˆ·åœ¨ç•Œé¢ä¸Šé€‰æ‹©å¤„ç†æ–¹å¼ï¼ˆæ‹¼æ¥æˆ–åˆ†åˆ«å¤„ç†ï¼‰
    """
    if not uploaded_files:
        st.error("è¯·å…ˆä¸Šä¼ æ–‡ä»¶")
        return None, None, None

    names_exts = ('.names', '.arff', '.doc')
    data_exts = ('.data', '.csv', '.txt', '.xlsx', '.xls', '.mat', '.arff', '.tsv', '.dat', '.tst')

    names_files = [f for f in uploaded_files
                   if os.path.splitext(f.name)[1].lower() in names_exts]
    data_files = [f for f in uploaded_files
                  if os.path.splitext(f.name)[1].lower() in data_exts]

    # å•æ–‡ä»¶ç›´æ¥è¯»å–
    if len(uploaded_files) == 1 and uploaded_files[0] in data_files:
        df = read_data_from_file(uploaded_files[0], col_names=None)
        return df, [df], [uploaded_files[0].name]

    if not data_files:
        raise ValueError(
            "æœªæ£€æµ‹åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶ï¼Œè¯·ä¸Šä¼ æ”¯æŒçš„æ ¼å¼ï¼š.csv/.data/.txt/.xlsx/.xls/.mat/.arff/.tsv/.dat/.tst"
        )

    # è¯»å–æ‰€æœ‰æ•°æ®æ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶ä½¿ç”¨è‡ªå·±çš„åˆ—å
    # å¦‚æœå­˜åœ¨è¡¨å¤´æ–‡ä»¶ï¼Œåªå¯¹ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶åº”ç”¨è¡¨å¤´
    dfs = []
    file_names = []
    
    for idx, data_file in enumerate(data_files):
        # å¦‚æœå­˜åœ¨è¡¨å¤´æ–‡ä»¶ä¸”æ˜¯ç¬¬ä¸€ä¸ªæ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨è¡¨å¤´æ–‡ä»¶çš„åˆ—å
        if names_files and idx == 0:
            sample_df = read_data_from_file(data_file, col_names=None)
            col_names = dataloadingagent.read_names_from_file(names_files[0], sample_df.head())
            df = read_data_from_file(data_file, col_names=col_names)
        else:
            # å…¶ä»–æ–‡ä»¶ä½¿ç”¨è‡ªå·±çš„åˆ—å
            df = read_data_from_file(data_file, col_names=None)
        
        dfs.append(df)
        file_names.append(data_file.name)

    # è¿”å›ç¬¬ä¸€ä¸ª DataFrame ä½œä¸ºé»˜è®¤æ˜¾ç¤ºï¼Œæ‰€æœ‰ DataFrame åˆ—è¡¨ï¼Œä»¥åŠæ–‡ä»¶åç§°åˆ—è¡¨
    # ä¸è¿›è¡Œè‡ªåŠ¨æ‹¼æ¥ï¼Œç”±ç”¨æˆ·åœ¨ç•Œé¢ä¸Šé€‰æ‹©å¤„ç†æ–¹å¼
    if len(dfs) == 1:
        return dfs[0], dfs, file_names
    else:
        # å¤šä¸ªæ–‡ä»¶æ—¶ï¼Œè¿”å›ç¬¬ä¸€ä¸ªæ–‡ä»¶ä½œä¸ºé»˜è®¤ï¼Œä½†ä¿ç•™æ‰€æœ‰æ–‡ä»¶ä¾›ç”¨æˆ·é€‰æ‹©
        return dfs[0], dfs, file_names


def load_from_path(local_path):

    ext = os.path.splitext(local_path)[1].lower()
    if ext in (".csv", ".txt", ".data"):
        df_local = pd.read_csv(local_path)
    elif ext in (".xls", ".xlsx"):
        df_local = pd.read_excel(local_path)
    elif ext == ".json":
        df_local = pd.read_json(local_path)
    elif ext == ".jsonl":
        df_local = pd.read_json(local_path, lines=True)
    elif ext == ".parquet":
        df_local = pd.read_parquet(local_path)
    elif ext in (".pkl", ".pickle"):
        df_local = pd.read_pickle(local_path)
    elif ext == ".feather":
        df_local = pd.read_feather(local_path)
    elif ext == ".arff":
        data, meta = arff.loadarff(local_path)
        df_local = pd.DataFrame(data)
        for col in df_local.select_dtypes([object]).columns:
            if isinstance(df_local[col].iloc[0], bytes):
                df_local[col] = df_local[col].str.decode('utf-8')
    else:
        st.error(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼š{ext}")
        df_local = None

    return df_local


def load_concat_file(dfs, agent, file_names=None):
    """
    å¤„ç†å¤šä¸ªæ•°æ®æ–‡ä»¶çš„é€‰æ‹©ç•Œé¢
    - æ‹¼æ¥ï¼šæ¨ªå‘æˆ–çºµå‘æ‹¼æ¥
    - åˆ†åˆ«å¤„ç†ï¼šé€‰æ‹©ä½¿ç”¨å“ªä¸ªæ–‡ä»¶
    """
    if file_names is None:
        file_names = [f"æ–‡ä»¶ {i+1}" for i in range(len(dfs))]
    
    st.info(f"æ£€æµ‹åˆ° {len(dfs)} ä¸ªæ•°æ®æ–‡ä»¶ï¼Œè¯·é€‰æ‹©å¤„ç†æ–¹å¼ï¼š")
    
    # æ˜¾ç¤ºæ–‡ä»¶ä¿¡æ¯
    with st.expander("ğŸ“ æŸ¥çœ‹æ–‡ä»¶ä¿¡æ¯", expanded=False):
        for idx, (df, name) in enumerate(zip(dfs, file_names)):
            st.write(f"**{name}**: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
            # å®‰å…¨åœ°å¤„ç†åˆ—åæ˜¾ç¤ºï¼Œé˜²æ­¢ç±»å‹é”™è¯¯
            try:
                columns_list = df.columns.tolist() if hasattr(df.columns, 'tolist') else list(df.columns)
                displayed_columns = ', '.join(columns_list[:5]) if columns_list else ''
                st.write(f"åˆ—å: {displayed_columns}{'...' if len(columns_list) > 5 else ''}")
            except Exception as e:
                st.write(f"åˆ—åæ˜¾ç¤ºé”™è¯¯: {str(e)}")
            if idx < len(dfs) - 1:
                st.divider()
    
    # ä½¿ç”¨ session_state æ¥è·Ÿè¸ªå¤„ç†æ¨¡å¼
    mode_key = f"concat_mode_{id(dfs)}"
    if mode_key not in st.session_state:
        st.session_state[mode_key] = 0  # é»˜è®¤é€‰æ‹©"åˆ†åˆ«å¤„ç†"
    
    mode = sac.segmented(
        items=[
            sac.SegmentedItem(label='åˆ†åˆ«å¤„ç†'),
            sac.SegmentedItem(label='çºµå‘æ‹¼æ¥'),
            sac.SegmentedItem(label='æ¨ªå‘æ‹¼æ¥'),
        ], 
        label='é€‰æ‹©å¤„ç†æ–¹å¼', 
        size='sm', 
        radius='sm', 
        index=st.session_state[mode_key]
    )
    
    # æ›´æ–° session_state ä»¥è·Ÿè¸ªå½“å‰é€‰æ‹©
    if mode == 'åˆ†åˆ«å¤„ç†':
        st.session_state[mode_key] = 0
    elif mode == 'çºµå‘æ‹¼æ¥':
        st.session_state[mode_key] = 1
    elif mode == 'æ¨ªå‘æ‹¼æ¥':
        st.session_state[mode_key] = 2

    if mode == 'åˆ†åˆ«å¤„ç†' or (isinstance(mode, str) and mode.startswith("åˆ†åˆ«å¤„ç†")):
        # è®©ç”¨æˆ·é€‰æ‹©ä½¿ç”¨å“ªä¸ªæ–‡ä»¶
        select_key = f"select_file_idx_{id(dfs)}"
        
        # ä½¿ç”¨ key å‚æ•°æ—¶ï¼ŒStreamlit ä¼šè‡ªåŠ¨ç®¡ç† session_stateï¼Œä¸éœ€è¦æ‰‹åŠ¨æ›´æ–°
        selected_idx = st.selectbox(
            "é€‰æ‹©è¦ä½¿ç”¨çš„æ•°æ®æ–‡ä»¶",
            options=range(len(dfs)),
            format_func=lambda x: f"{file_names[x]} ({dfs[x].shape[0]} è¡Œ Ã— {dfs[x].shape[1]} åˆ—)",
            index=0,  # é»˜è®¤é€‰æ‹©ç¬¬ä¸€ä¸ªæ–‡ä»¶
            key=select_key
        )
        
        selected_df = dfs[selected_idx]
        agent.add_df(selected_df)
        st.success(f"å·²é€‰æ‹©ä½¿ç”¨ï¼š{file_names[selected_idx]}")
        
    elif mode == 'æ¨ªå‘æ‹¼æ¥' or (isinstance(mode, str) and mode.startswith("æ¨ªå‘æ‹¼æ¥")):
        # æ¨ªå‘æ‹¼æ¥ï¼šè¦æ±‚è¡Œæ•°ç›¸åŒ
        try:
            dfs_pos = [df.reset_index(drop=True) for df in dfs]
            big_df = pd.concat(dfs_pos, axis=1)
            
            # å¤„ç†é‡å¤åˆ—å
            cols = []
            seen = {}
            for c in big_df.columns:
                if c in seen:
                    seen[c] += 1
                    cols.append(f"{c}_{seen[c]}")
                else:
                    seen[c] = 0
                    cols.append(c)
            big_df.columns = cols
            agent.add_df(big_df)
            st.success(f"æ¨ªå‘æ‹¼æ¥å®Œæˆï¼š{big_df.shape[0]} è¡Œ Ã— {big_df.shape[1]} åˆ—")
        except Exception as e:
            st.error(f"æ¨ªå‘æ‹¼æ¥å¤±è´¥ï¼š{str(e)}ã€‚è¯·ç¡®ä¿æ‰€æœ‰æ–‡ä»¶çš„è¡Œæ•°ç›¸åŒã€‚")
            return
            
    else:  # çºµå‘æ‹¼æ¥
        # çºµå‘æ‹¼æ¥ï¼šè¦æ±‚åˆ—åç›¸åŒæˆ–å…¼å®¹
        try:
            big_df = pd.concat(dfs, axis=0, ignore_index=True)
            agent.add_df(big_df)
            st.success(f"çºµå‘æ‹¼æ¥å®Œæˆï¼š{big_df.shape[0]} è¡Œ Ã— {big_df.shape[1]} åˆ—")
        except Exception as e:
            st.warning(f"çºµå‘æ‹¼æ¥æ—¶å‡ºç°è­¦å‘Šï¼š{str(e)}ã€‚å°è¯•ç»Ÿä¸€åˆ—ååæ‹¼æ¥...")
            # å°è¯•ç»Ÿä¸€åˆ—ååæ‹¼æ¥
            try:
                # è·å–æ‰€æœ‰åˆ—åçš„å¹¶é›†
                all_cols = set()
                for df in dfs:
                    all_cols.update(df.columns)
                all_cols = sorted(list(all_cols))
                
                # ä¸ºæ¯ä¸ª DataFrame æ·»åŠ ç¼ºå¤±çš„åˆ—
                dfs_aligned = []
                for df in dfs:
                    df_aligned = df.copy()
                    for col in all_cols:
                        if col not in df_aligned.columns:
                            df_aligned[col] = None
                    dfs_aligned.append(df_aligned[all_cols])
                
                big_df = pd.concat(dfs_aligned, axis=0, ignore_index=True)
                agent.add_df(big_df)
                st.success(f"çºµå‘æ‹¼æ¥å®Œæˆï¼ˆå·²ç»Ÿä¸€åˆ—åï¼‰ï¼š{big_df.shape[0]} è¡Œ Ã— {big_df.shape[1]} åˆ—")
            except Exception as e2:
                st.error(f"çºµå‘æ‹¼æ¥å¤±è´¥ï¼š{str(e2)}ã€‚å»ºè®®ä½¿ç”¨ã€Œåˆ†åˆ«å¤„ç†ã€é€‰é¡¹ã€‚")
                return

    # ä¸‹è½½æŒ‰é’®
    current_df = agent.load_df()
    if current_df is not None:
        csv_bytes = current_df.to_csv(index=False).encode('utf-8')
        st.download_button(
            label="ä¸‹è½½å¤„ç†åçš„æ–‡ä»¶",
            data=csv_bytes,
            file_name="processed_data.csv",
            mime="text/csv"
        )


class PathFileWrapper:
    """A wrapper to treat a local file path as a Streamlit UploadedFile."""
    def __init__(self, path):
        self.path = path
        self.name = os.path.basename(path)
        self._file = None

    def read(self, *args, **kwargs):
        with open(self.path, 'rb') as f:
            return f.read()

    def seek(self, offset, whence=0):

        pass

    def __repr__(self):
        return f"PathFileWrapper(path='{self.path}')"