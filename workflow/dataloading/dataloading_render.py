import os
from typing import List, Optional

import pandas as pd
import streamlit as st
import streamlit_antd_components as sac

from workflow.dataloading.dataloading_core import process_complex_data, load_from_path, load_concat_file, PathFileWrapper

def loading_reference_docs(agent):
    """
    ä¸“é—¨å¤„ç†å‚è€ƒèµ„æ–™çš„ä¸Šä¼ é€»è¾‘
    """
    st.info("ğŸ’¡ æç¤ºï¼šåœ¨æ­¤å¤„ä¸Šä¼ ä¸šåŠ¡èƒŒæ™¯ã€ç®—æ³•è¯´æ˜æˆ–æ•°æ®æ‰‹å†Œ (PDF/Docx)ï¼ŒAI ä¼šå­¦ä¹ è¿™äº›å†…å®¹ã€‚")
    
    # 2. æ–‡ä»¶ä¸Šä¼ ç»„ä»¶
    uploaded_docs = st.file_uploader(
        "ä¸Šä¼ å‚è€ƒæ–‡æ¡£",
        type=['pdf', 'docx', 'txt', 'names'],
        accept_multiple_files=True,
        key="ref_doc_uploader" # è®¾ç½® key é˜²æ­¢ä¸æ•°æ®ä¸Šä¼ å†²çª
    )

    if uploaded_docs:
        # è®°å½•å·²å¤„ç†è¿‡çš„æ–‡ä»¶åï¼Œé¿å…é‡å¤å­¦ä¹ 
        if 'learned_doc_names' not in st.session_state:
            st.session_state.learned_doc_names = set()

        new_files = [f for f in uploaded_docs if f.name not in st.session_state.learned_doc_names]
        
        if new_files:
            if st.button("ğŸ§  å­¦ä¹ é€‰ä¸­çš„èµ„æ–™", use_container_width=True):
                with st.spinner("æ­£åœ¨è§£ææ–‡æ¡£å¹¶æå–çŸ¥è¯†ç‚¹..."):
                    count = st.session_state.retriever.add_uploaded_files(new_files)
                    for f in new_files:
                        st.session_state.learned_doc_names.add(f.name)
                st.success(f"å­¦ä¹ æˆåŠŸï¼æ–°å¢ {len(new_files)} ä»½æ–‡æ¡£ï¼Œæå–äº† {count} æ¡çŸ¥è¯†ç‰‡æ®µã€‚")
        else:
            st.caption("âœ… å½“å‰ä¸Šä¼ çš„æ–‡ä»¶å·²å…¨éƒ¨åœ¨çŸ¥è¯†åº“ä¸­ã€‚")

    # å±•ç¤ºå·²åŠ è½½çš„æ–‡æ¡£åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
    if 'learned_doc_names' in st.session_state and st.session_state.learned_doc_names:
        with st.expander("æŸ¥çœ‹å½“å‰å·²åŠ è½½çš„å¤–éƒ¨èµ„æ–™"):
            for name in st.session_state.learned_doc_names:
                st.write(f"- ğŸ“„ {name}")
def loading_data_file(agent):

    st.info(
        "ğŸ’¡ æç¤ºï¼š\n"
        "1. æ”¯æŒä¸€æ¬¡ä¸Šä¼ å¤šä¸ªæ•°æ®æ–‡ä»¶\n"
        "2. è‡ªåŠ¨ä½¿ç”¨å¤§æ¨¡å‹åˆ†æå¹¶å¤„ç†æ•°æ®\n"
        "3. æ”¯æŒå¤šç§æ ¼å¼çš„æ–‡ä»¶ç±»å‹ä¸Šä¼ \n"
    )

    selected_index = sac.tabs([
        sac.TabsItem(label='æœ¬åœ°ä¸Šä¼ '),
        sac.TabsItem(label='è·¯å¾„å¯¼å…¥'),
    ], color='#5980AE',)

    if selected_index == "æœ¬åœ°ä¸Šä¼ ":
        # ç‚¹å‡»ä¸Šä¼ æ–‡ä»¶
        uploaded_files = st.file_uploader(
            "é€‰æ‹©æ–°æ–‡ä»¶",
            accept_multiple_files=True,
            help="æ‹–æ‹½æˆ–ç‚¹å‡»ä¸Šä¼ å¤šä¸ªæ–‡ä»¶",
        )

        if uploaded_files:
            current_memory_file_name = agent.load_file_name()
            new_files = [f for f in uploaded_files if f.name not in current_memory_file_name]
            if new_files:
                try:
                    with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                        df, dfs = process_complex_data(new_files, agent)
                    if df is not None:
                        agent.add_df(df)
                        agent.save_dfs(dfs)
                        for f in new_files:
                            agent.save_file_name(f.name)
                        st.rerun()
                except Exception as err:
                    st.error(f"å¯¼å…¥å¤±è´¥ï¼š{err}")

    elif selected_index == "è·¯å¾„å¯¼å…¥":
        # è·¯å¾„ä¸Šä¼ æ–‡ä»¶
        raw_paths = st.text_area(
            "ä»è·¯å¾„å¯¼å…¥æ•°æ® (æ¯è¡Œä¸€ä¸ªæ–‡ä»¶è·¯å¾„)",
            placeholder=    "C:\\data\\iris.names\nC:\\data\\iris.data",
            height=100
        )

        if st.button("ä»è·¯å¾„åŠ è½½æ–‡ä»¶", use_container_width=True):
            if raw_paths:

                path_list = [p.strip().strip("'\"") for p in raw_paths.strip().split('\n') if p.strip()]
                
                valid_paths = [p for p in path_list if os.path.exists(p)]
                invalid_paths = [p for p in path_list if not os.path.exists(p)]

                if invalid_paths:
                    st.warning(f"è·¯å¾„ä¸å­˜åœ¨ï¼Œå·²è·³è¿‡ï¼š\n- " + "\n- ".join(invalid_paths))

                if not valid_paths:
                    st.error("æœªæ‰¾åˆ°ä»»ä½•æœ‰æ•ˆçš„æœ¬åœ°æ–‡ä»¶è·¯å¾„ã€‚")
                else:
                    current_memory_file_name = agent.load_file_name()
                    new_paths = [p for p in valid_paths if p not in current_memory_file_name]

                    if not new_paths:
                        st.info("æ‰€æœ‰æŒ‡å®šçš„è·¯å¾„æ–‡ä»¶å‡å·²åŠ è½½ã€‚")
                    else:
                        files_to_process = [PathFileWrapper(p) for p in new_paths]
                        try:
                            with st.spinner("æ­£åœ¨å¤„ç†æ•°æ®..."):
                                df, dfs = process_complex_data(files_to_process, agent)
                            if df is not None:
                                agent.add_df(df)
                                agent.save_dfs(dfs)
                                for p in new_paths:
                                    agent.save_file_name(p)
                                st.rerun()
                        except Exception as err:
                            st.error(f"æœ¬åœ°æ–‡ä»¶è¯»å–å¤±è´¥ï¼š{err}")
    
    dfs = agent.load_dfs()
    if dfs is not None and len(dfs) >= 2:
        load_concat_file(dfs, agent)


def loading_basic_info(agent):
    
    df = agent.load_df()
    if df is not None:
        r, c = df.shape
        missing = int(df.isnull().sum().sum())
        col1, col2, col3 = st.columns(3)
        col1.metric("è¡Œæ•°", r)
        col2.metric("åˆ—æ•°", c)
        col3.metric("ç¼ºå¤±å€¼æ€»æ•°", missing)

        dtype_info = pd.DataFrame({
            "åˆ—å": df.columns,
            "ç±»å‹": df.dtypes.astype(str),
            "éç©º": df.count().values,
            "ç¼ºå¤±%": (df.isnull().mean() * 100).round(2).values,
        }).reset_index(drop=True)

        selected_index = sac.tabs([
            sac.TabsItem(label='æ•°æ®ç±»å‹æ¦‚è§ˆ'),
            sac.TabsItem(label='æ•°æ®é¢„è§ˆ'),
        ],color='#5980AE',)

        if selected_index == "æ•°æ®ç±»å‹æ¦‚è§ˆ":
            st.dataframe(dtype_info, use_container_width=True)
        elif selected_index == "æ•°æ®é¢„è§ˆ":
            if st.button("ğŸ² éšæœºæŠ½æ ·"):
                display_df = df.sample(10)
                st.dataframe(display_df, use_container_width=True)
            else:
                st.dataframe(df.head(10), use_container_width=True)


def loading_chat(agent, auto=False) -> None:

    df = agent.load_df()
    if df is None:
        return

    with st.chat_message("assistant"):
        st.write(
            "æˆ‘æ˜¯ Autostat æ•°æ®åˆ†æåŠ©æ‰‹ï¼Œå¾ˆé«˜å…´ä¸ºæ‚¨æœåŠ¡ï¼\n\n"
            "è¯·å…ˆä¸Šä¼ æ‚¨çš„æ•°æ®æ–‡ä»¶ï¼Œä¸Šä¼ å®Œæˆåï¼Œæ‚¨å¯ä»¥åœ¨ä¸‹æ–¹å’Œæˆ‘å¯¹è¯ï¼Œä¹Ÿå¯ä»¥ç›´æ¥ç‚¹å‡»æŒ‰é’®è§£ææ•°æ®å«ä¹‰ã€‚"
        )
        analyze_btn = st.button("ğŸ” è§£æå«ä¹‰")
        result_placeholder = st.empty()
        
    # æ¸²æŸ“å†å²å¯¹è¯
    chat_history = agent.load_memory()

    for idx, entry in enumerate(chat_history):
        bubble = st.chat_message(entry["role"])
        content = entry["content"]
        if isinstance(content, str):
            bubble.write(content)

    already_generated = any(
        entry["role"] == "assistant" and "å«ä¹‰" in str(entry["content"])
        for entry in chat_history
    )

    if analyze_btn or (auto and not already_generated):
        st.chat_message("user").write("è¯·å¸®æˆ‘è§£ææ•°æ®å«ä¹‰")
        agent.add_memory({"role": "user", "content": "è¯·å¸®æˆ‘è§£ææ•°æ®å«ä¹‰"})
        with st.spinner("åˆ†æä¸­..."):
            desc = agent.do_data_description(df)

        agent.finish_auto()
        st.chat_message("assistant").write(desc)
        agent.add_memory({"role": "assistant", "content": desc})
        st.rerun()

    # ç”¨æˆ·è‡ªå®šä¹‰è¾“å…¥
    user_input = st.chat_input("è¯·è¾“å…¥éœ€æ±‚ï¼Œä¾‹å¦‚â€œå¸®æˆ‘åˆ†æxxåˆ—â€")
    if user_input:
        st.chat_message("user").write(user_input)
        agent.add_memory({"role": "user", "content": user_input})
        with st.spinner("å¤„ç†ä¸­â€¦"):
            reply = agent.do_data_description(df, user_input)

        st.chat_message("assistant").write(reply)
        agent.add_memory({"role": "assistant", "content": reply})
        st.rerun()


if __name__ == "__main__":

    agent = st.session_state.data_loading_agent
    planner = st.session_state.planner_agent
    auto = planner.loading_auto

    if st.session_state.auto_mode == True:
        if (agent.finish_auto_task == True and planner.switched_prep == False) or planner.loading_auto == False:
            planner.finish_loading_auto()
            st.switch_page("workflow/preprocessing/preprocessing_render.py")

    c1,c2 = st.columns(2)
    with c1:
        st.title("æ•°æ®å¯¼å…¥")
    with c2:
        st.write("")  
        st.write("")  
        sac.buttons([
            sac.ButtonsItem(label='Github', icon='github', href='https://github.com/Automated-Statistician/AutoSTAT'),
            sac.ButtonsItem(label='Doc', icon=sac.BsIcon(name='bi bi-file-earmark-post-fill', size=16), href='https://automated-statistician.github.io/autostatdoc.github.io/'),
        ], align='end', color='dark', variant='filled', index=None)
    st.markdown("---")

    c = st.columns(3)
    with c[0].expander('æ•°æ®ä¸Šä¼ ', True):
        loading_data_file(agent)
    with c[0].expander('æ•°æ®å±•ç¤º', True):
        loading_basic_info(agent)
    with c[1].expander('å‚è€ƒèµ„æ–™pdf/docxä¸Šä¼ ', True):
        loading_reference_docs(agent)
    with c[2].expander('æ•°æ®å»ºè®®', True):
        loading_chat(agent, auto)
